{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bq_helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-519afc9e4434>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mbq_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbq_helper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBigQueryHelper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# https://www.kaggle.com/sohier/introduction-to-the-bq-helper-package\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m noaa = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",\n\u001b[0;32m      5\u001b[0m                                    dataset_name=\"ghcn_d\")\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bq_helper'"
     ]
    }
   ],
   "source": [
    "import bq_helper\n",
    "from bq_helper import BigQueryHelper\n",
    "# https://www.kaggle.com/sohier/introduction-to-the-bq-helper-package\n",
    "noaa = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",\n",
    "                                   dataset_name=\"ghcn_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1c250e43cbfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Helper class to simplify common read-only BigQuery tasks.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "class BigQueryHelper(object):\n",
    "    \"\"\"\n",
    "    Helper class to simplify common BigQuery tasks like executing queries,\n",
    "    showing table schemas, etc without worrying about table or dataset pointers.\n",
    "    See the BigQuery docs for details of the steps this class lets you skip:\n",
    "    https://googlecloudplatform.github.io/google-cloud-python/latest/bigquery/reference.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, active_project, dataset_name, max_wait_seconds=180):\n",
    "        self.project_name = active_project\n",
    "        self.dataset_name = dataset_name\n",
    "        self.max_wait_seconds = max_wait_seconds\n",
    "        self.client = bigquery.Client()\n",
    "        self.__dataset_ref = self.client.dataset(self.dataset_name, project=self.project_name)\n",
    "        self.dataset = None\n",
    "        self.tables = dict()  # {table name (str): table object}\n",
    "        self.__table_refs = dict()  # {table name (str): table reference}\n",
    "        self.total_gb_used_net_cache = 0\n",
    "        self.BYTES_PER_GB = 2**30\n",
    "\n",
    "    def __fetch_dataset(self):\n",
    "        \"\"\"\n",
    "        Lazy loading of dataset. For example,\n",
    "        if the user only calls `self.query_to_pandas` then the\n",
    "        dataset never has to be fetched.\n",
    "        \"\"\"\n",
    "        if self.dataset is None:\n",
    "            self.dataset = self.client.get_dataset(self.__dataset_ref)\n",
    "\n",
    "    def __fetch_table(self, table_name):\n",
    "        \"\"\"\n",
    "        Lazy loading of table\n",
    "        \"\"\"\n",
    "        self.__fetch_dataset()\n",
    "        if table_name not in self.__table_refs:\n",
    "            self.__table_refs[table_name] = self.dataset.table(table_name)\n",
    "        if table_name not in self.tables:\n",
    "            self.tables[table_name] = self.client.get_table(self.__table_refs[table_name])\n",
    "\n",
    "    def __handle_record_field(self, row, schema_details, top_level_name=''):\n",
    "        \"\"\"\n",
    "        Unpack a single row, including any nested fields.\n",
    "        \"\"\"\n",
    "        name = row['name']\n",
    "        if top_level_name != '':\n",
    "            name = top_level_name + '.' + name\n",
    "        schema_details.append([{\n",
    "            'name': name,\n",
    "            'type': row['type'],\n",
    "            'mode': row['mode'],\n",
    "            'fields': pd.np.nan,\n",
    "            'description': row['description']\n",
    "                               }])\n",
    "        # float check is to dodge row['fields'] == np.nan\n",
    "        if type(row.get('fields', 0.0)) == float:\n",
    "            return None\n",
    "        for entry in row['fields']:\n",
    "            self.__handle_record_field(entry, schema_details, name)\n",
    "\n",
    "    def __unpack_all_schema_fields(self, schema):\n",
    "        \"\"\"\n",
    "        Unrolls nested schemas. Returns dataframe with one row per field,\n",
    "        and the field names in the format accepted by the API.\n",
    "        Results will look similar to the website schema, such as:\n",
    "            https://bigquery.cloud.google.com/table/bigquery-public-data:github_repos.commits?pli=1\n",
    "        Args:\n",
    "            schema: DataFrame derived from api repr of raw table.schema\n",
    "        Returns:\n",
    "            Dataframe of the unrolled schema.\n",
    "        \"\"\"\n",
    "        schema_details = []\n",
    "        schema.apply(lambda row:\n",
    "            self.__handle_record_field(row, schema_details), axis=1)\n",
    "        result = pd.concat([pd.DataFrame.from_dict(x) for x in schema_details])\n",
    "        result.reset_index(drop=True, inplace=True)\n",
    "        del result['fields']\n",
    "        return result\n",
    "\n",
    "    def table_schema(self, table_name):\n",
    "        \"\"\"\n",
    "        Get the schema for a specific table from a dataset.\n",
    "        Unrolls nested field names into the format that can be copied\n",
    "        directly into queries. For example, for the `github.commits` table,\n",
    "        the this will return `committer.name`.\n",
    "        This is a very different return signature than BigQuery's table.schema.\n",
    "        \"\"\"\n",
    "        self.__fetch_table(table_name)\n",
    "        raw_schema = self.tables[table_name].schema\n",
    "        schema = pd.DataFrame.from_dict([x.to_api_repr() for x in raw_schema])\n",
    "        # the api_repr only has the fields column for tables with nested data\n",
    "        if 'fields' in schema.columns:\n",
    "            schema = self.__unpack_all_schema_fields(schema)\n",
    "        # Set the column order\n",
    "        schema = schema[['name', 'type', 'mode', 'description']]\n",
    "        return schema\n",
    "\n",
    "    def list_tables(self):\n",
    "        \"\"\"\n",
    "        List the names of the tables in a dataset\n",
    "        \"\"\"\n",
    "        self.__fetch_dataset()\n",
    "        return([x.table_id for x in self.client.list_tables(self.dataset)])\n",
    "\n",
    "    def estimate_query_size(self, query):\n",
    "        \"\"\"\n",
    "        Estimate gigabytes scanned by query.\n",
    "        Does not consider if there is a cached query table.\n",
    "        See https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.dryRun\n",
    "        \"\"\"\n",
    "        my_job_config = bigquery.job.QueryJobConfig()\n",
    "        my_job_config.dry_run = True\n",
    "        my_job = self.client.query(query, job_config=my_job_config)\n",
    "        return my_job.total_bytes_processed / self.BYTES_PER_GB\n",
    "\n",
    "    def query_to_pandas(self, query):\n",
    "        \"\"\"\n",
    "        Execute a SQL query & return a pandas dataframe\n",
    "        \"\"\"\n",
    "        my_job = self.client.query(query)\n",
    "        start_time = time.time()\n",
    "        while not my_job.done():\n",
    "            if (time.time() - start_time) > self.max_wait_seconds:\n",
    "                print(\"Max wait time elapsed, query cancelled.\")\n",
    "                self.client.cancel_job(my_job.job_id)\n",
    "                return None\n",
    "            time.sleep(0.1)\n",
    "        # Queries that hit errors will return an exception type.\n",
    "        # Those exceptions don't get raised until we call my_job.to_dataframe()\n",
    "        # In that case, my_job.total_bytes_billed can be called but is None\n",
    "        if my_job.total_bytes_billed:\n",
    "            self.total_gb_used_net_cache += my_job.total_bytes_billed / self.BYTES_PER_GB\n",
    "        return my_job.to_dataframe()\n",
    "\n",
    "    def query_to_pandas_safe(self, query, max_gb_scanned=1):\n",
    "        \"\"\"\n",
    "        Execute a query, but only if the query would scan less than `max_gb_scanned` of data.\n",
    "        \"\"\"\n",
    "        query_size = self.estimate_query_size(query)\n",
    "        if query_size <= max_gb_scanned:\n",
    "            return self.query_to_pandas(query)\n",
    "        msg = \"Query cancelled; estimated size of {0} exceeds limit of {1} GB\"\n",
    "        print(msg.format(query_size, max_gb_scanned))\n",
    "\n",
    "    def head(self, table_name, num_rows=5, start_index=None, selected_columns=None):\n",
    "        \"\"\"\n",
    "        Get the first n rows of a table as a DataFrame.\n",
    "        Does not perform a full table scan; should use a trivial amount of data as long as n is small.\n",
    "        \"\"\"\n",
    "        self.__fetch_table(table_name)\n",
    "        active_table = self.tables[table_name]\n",
    "        schema_subset = None\n",
    "        if selected_columns:\n",
    "            schema_subset = [col for col in active_table.schema if col.name in selected_columns]\n",
    "        results = self.client.list_rows(active_table, selected_fields=schema_subset,\n",
    "            max_results=num_rows, start_index=start_index)\n",
    "        results = [x for x in results]\n",
    "        return pd.DataFrame(\n",
    "            data=[list(x.values()) for x in results], columns=list(results[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
